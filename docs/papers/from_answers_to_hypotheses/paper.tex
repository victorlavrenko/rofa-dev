\documentclass[11pt]{article}

\usepackage{times}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{margin=1in}

\title{From Answers to Hypotheses:\\Internal Consensus and Its Limits}

\author{
Victor Lavrenko\\
\texttt{victor@peacetech.vc}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Recent advances in large language models (LLMs) have demonstrated expert-level performance on medical question answering and diagnostic benchmarks, including recent near-clinical diagnostic systems and evaluations reported in 2024--2025, in some cases matching or exceeding average physician performance on structured tasks \cite{wang2022selfconsistency,kadavath2022know,tu2025amie}. At the same time, multiple studies have documented a persistent gap between benchmark accuracy and safe real-world deployment, largely driven by miscalibrated confidence and coherent but incorrect reasoning---failure modes that are especially dangerous in high-stakes domains such as medicine.

Building on prior work on self-consistency and internal uncertainty estimation, we argue that the core limitation is not lack of knowledge, but a failure to properly surface and reason over the \emph{distribution of internal hypotheses} generated by the model. Using controlled multi-branch sampling on a medical QA benchmark, we empirically demonstrate three results: (i) majority-vote aggregation does not yield statistically significant accuracy improvements over greedy decoding; (ii) the correct answer is frequently present among alternative hypotheses even when the final prediction is wrong; and (iii) strong internal consensus does not imply correctness.

We formalize these findings using explicit statistical tests and propose an alternative failure mode---\emph{errors of hypothesis space}---where the model converges confidently on a systematically wrong explanation. The results motivate a shift from answer-centric evaluation toward hypothesis-centric analysis, analogous to differential diagnosis in clinical medicine.
\end{abstract}

\section{Introduction}

Large language models have recently achieved impressive results on medical examinations and diagnostic benchmarks, in some cases rivaling or exceeding average physician performance. However, randomized studies and post-deployment analyses suggest that these gains do not reliably translate into improved clinical decision-making and may introduce new risks when model outputs are treated as definitive answers.

Kadavath et al.~\cite{kadavath2022know} show that language models often know when they are likely to be wrong, but this knowledge is not reliably exposed through standard decoding. Wang et al.~\cite{wang2022selfconsistency} demonstrate that sampling multiple reasoning traces and aggregating them via self-consistency can improve accuracy on some reasoning tasks.

Recent clinical and near-clinical evaluations further underscore the gap between benchmark performance and dependable decision support. In a randomized controlled study, access to an LLM did not significantly improve physicians' diagnostic reasoning compared to conventional resources \cite{goh2024jama}. At the same time, frontier systems such as AMIE demonstrate rapid progress toward conversational diagnostic reasoning \cite{tu2025amie,mcduff2025naturemed}.

This work studies LLM reasoning as a \emph{distribution over hypotheses}, rather than a single chain of thought. Operationally, this distribution is observed through repeated sampling of the same prompt under fixed conditions, yielding an empirical distribution over final answers. By ``hypotheses'' we refer to discrete answer-level explanations instantiated by independent decoding trajectories, not to explicit symbolic representations.

We emphasize that this work is diagnostic rather than prescriptive: we do not propose a new aggregation or verification algorithm, but instead characterize when and why aggregation and internal agreement fail.

\section{Experimental Setup}

For each question, we generate a greedy prediction and an ensemble of $N=10$ independently sampled reasoning paths. Let $a_{i,j}$ denote the final answer of branch $j$ for question $i$. From the empirical distribution $P_i(a)$ we compute:
\begin{align}
\text{max\_frac}_i &= \max_a P_i(a), \\
\text{Top-}k &= \mathbb{1}\{ y_i \in \text{Top-}k(P_i) \}.
\end{align}

All results are computed over 400 questions using a fixed prompt and model configuration.

\section{Hypothesis H1: Aggregation Improves Accuracy}

Majority-vote aggregation yields an accuracy of 66.75\%, compared to 65.75\% for greedy decoding. A two-sided binomial test yields $p \approx 0.63$, indicating no statistically significant improvement.

\section{Hypothesis H2: Correct Answers Appear Among Alternatives}

Observed Top-2 coverage is 80.5\%, compared to a greedy accuracy of 65.75\%, corresponding to an absolute improvement of 14.75 percentage points. Using a binomial test with null success probability equal to the greedy accuracy ($\pi = 0.6575$), this difference is highly statistically significant ($p$-value $\ll 10^{-6}$).

\section{Hypothesis H3: Internal Consensus Implies Correctness}

Unanimous cases achieve an accuracy of 86.8\%. Testing against a conservative reliability threshold of 95\% yields $p < 0.01$, allowing us to reject the hypothesis that strong internal agreement implies correctness.

\section{Distributional Analysis of Consensus}

Figure~\ref{fig:consensus} shows accuracy as a function of internal consensus. Accuracy increases monotonically with agreement but saturates well below perfect reliability. Even near-unanimous cases exhibit non-zero error rates.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{figure1_max_frac_exact.png}
\caption{Accuracy as a function of internal consensus (max\_frac). Accuracy increases monotonically with agreement but saturates well below perfect reliability.}
\label{fig:consensus}
\end{figure}

\section{Errors of Hypothesis Space}

We identify three failure modes: selection errors, calibration errors, and hypothesis space errors. Only the first is addressable through improved aggregation; the latter two require changes to training objectives or hypothesis representation.

\section{Conclusion}

These results suggest that safe deployment of LLMs in medicine requires decision rules that operate on sets of hypotheses rather than single answers.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
