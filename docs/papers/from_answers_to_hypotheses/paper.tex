\documentclass[11pt]{article}

% Fonts / encodings (pdfLaTeX)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Mathematics
\usepackage{amsmath,amssymb}

% Modern Times-like text + matching math
\usepackage{newtxtext}
\usepackage{newtxmath}

% Typography
\usepackage{microtype}
\usepackage{parskip}


% Figures / tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}

% Page layout
\usepackage[a4paper,margin=1in]{geometry}

% Captions
\usepackage[font=small,labelfont=bf]{caption}

% Bibliography
\usepackage{csquotes}
\usepackage[
  backend=biber,
  style=authoryear,
  labeldate=year,
  maxcitenames=2,  
  maxbibnames=99,
  doi=true,
  url=true,
  eprint=true,
  hyperref=true
]{biblatex}
\setlength{\bibitemsep}{0.6\baselineskip}
\addbibresource{../references.bib}
\usepackage[hidelinks]{hyperref}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}


% ----------------------------
% Full clickable author–year citations (name + year), biblatex authoryear
\DeclareFieldFormat{citehyperref}{%
  \DeclareFieldAlias{bibhyperref}{noformat}% avoid nested links
  \bibhyperref{#1}%
}

\DeclareFieldFormat{textcitehyperref}{%
  \DeclareFieldAlias{bibhyperref}{noformat}% avoid nested links
  \bibhyperref{%
    #1%
    \ifbool{cbx:parens}
      {\bibcloseparen\global\boolfalse{cbx:parens}}
      {}%
  }%
}

\savebibmacro{cite}
\savebibmacro{textcite}

\renewbibmacro*{cite}{%
  \printtext[citehyperref]{%
    \restorebibmacro{cite}%
    \usebibmacro{cite}%
  }%
}

\renewbibmacro*{textcite}{%
  \ifboolexpr{
    (not test {\iffieldundef{prenote}} and
      test {\ifnumequal{\value{citecount}}{1}})
    or
    (not test {\iffieldundef{postnote}} and
      test {\ifnumequal{\value{citecount}}{\value{citetotal}}})
  }
    {\DeclareFieldAlias{textcitehyperref}{noformat}}
    {}%
  \printtext[textcitehyperref]{%
    \restorebibmacro{textcite}%
    \usebibmacro{textcite}%
  }%
}

% -----------------------------------------
% Priority: arXiv (eprint) > DOI > URL
% -----------------------------------------
\renewbibmacro*{doi+eprint+url}{%
  \iffieldundef{eprint}
    {%
      \iffieldundef{doi}
        {%
          \iffieldundef{url}
            {}
            {\printfield{url}}%
        }
        {\printfield{doi}}%
    }
    {\usebibmacro{eprint}}%
}

\newcommand{\extlink}{\raisebox{0.1ex}{\tiny$\nearrow$}}


\title{%
From Answers to Hypotheses\\[0.5ex]
\large\itshape{}
Internal Consensus and Its Limits in Large Language Models
}

\author{
Victor Lavrenko\\
\texttt{victor@peacetech.vc}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Large language models (LLMs) now reach near-expert performance on medical QA and diagnostic benchmarks, yet this progress does not reliably translate into safe clinical decision support. We argue that a core limitation is not missing knowledge, but a failure to surface and reason over the \emph{distribution of internal hypotheses} expressed across alternative decoding trajectories. Using controlled multi-branch sampling on a single medical QA benchmark with one model, one prompt, and a fixed sampling regime, we show that (i) majority-vote self-consistency does not yield statistically significant accuracy gains over greedy decoding in this setting; (ii) the correct answer is often present among sampled hypotheses even when the final prediction is wrong; and (iii) strong internal consensus can still coincide with incorrect answers. We formalize these effects with statistical tests and offer \emph{errors of hypothesis space} as an empirically motivated interpretation---not a formally isolated mechanism---where models converge confidently on a systematically wrong explanation, motivating hypothesis-centric evaluation analogous to differential diagnosis.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have recently achieved impressive results on medical examinations and diagnostic benchmarks, in some cases rivaling or exceeding average physician performance on structured question answering and exam-style tasks. This progress has fueled optimism about near-term clinical deployment, with several systems now approaching conversational history-taking, differential diagnosis, and longitudinal reasoning. However, randomized studies and post-deployment analyses increasingly suggest that benchmark gains do not reliably translate into improved clinical decision-making and may introduce new risks when model outputs are treated as definitive answers.

A dominant explanation for this gap attributes failures to hallucinations and overconfidence. While these phenomena are real, prior work paints a more nuanced picture of model uncertainty.\ \textcite{kadavath2022know} show that language models often \emph{know when they are likely to be wrong}, in the sense that internal signals correlate with error, but this knowledge is not reliably exposed through standard decoding. Similarly, \textcite{wang2022selfconsistency} demonstrate that sampling multiple reasoning traces and aggregating them via self-consistency can improve accuracy on some reasoning benchmarks, suggesting that alternative hypotheses already exist within the model’s latent reasoning process.

Recent clinical and near-clinical evaluations further underscore the limits of treating improved benchmark accuracy as a proxy for dependable decision support. In a randomized controlled study, access to an LLM did not significantly improve physicians’ diagnostic reasoning compared to conventional resources \parencite{goh2024jama}, despite strong standalone model performance. At the same time, frontier systems such as AMIE illustrate rapid progress toward conversational diagnostic assistance and differential diagnosis \parencite{tu2025amie,mcduff2025naturemed}, amplifying the importance of understanding when internally coherent reasoning trajectories nonetheless converge on incorrect conclusions.

Taken together, these findings suggest that uncertainty and alternative explanations are not absent from modern LLMs; rather, they are often \emph{latent}. The open question is whether these internal structures are sufficiently reliable, diverse, and accessible to support high-stakes decision-making. In medicine, uncertainty is rarely resolved by selecting a single most likely hypothesis; instead, clinicians reason over \emph{sets} of competing explanations, iteratively revising them as new evidence emerges. This work adopts that perspective and studies LLM reasoning as a \emph{distribution over hypotheses}, rather than as a single chain of thought or a single decoded answer.

Operationally, we observe this distribution through repeated sampling of the same prompt under fixed conditions, yielding an empirical distribution over final answers and associated explanations. We do not assume access to internal symbolic representations or latent states; rather, by ``hypotheses'' we refer to the set of discrete answer-level explanations implicitly instantiated by independent decoding trajectories. This framing allows us to analyze internal agreement, diversity, and failure modes without introducing new architectures or training procedures.

Finally, we emphasize that this work is diagnostic rather than prescriptive. We do not propose a new aggregation rule, verification strategy, or decoding algorithm. Instead, our goal is to characterize when and why aggregation and internal agreement fail, even under idealized sampling conditions, and to identify a distinct failure mode---\emph{errors of hypothesis space}---as an empirically motivated interpretation in which models converge confidently on a systematically wrong explanation. Understanding these failures is a necessary step toward evaluation paradigms that more closely resemble differential diagnosis in clinical practice.

In this work, we evaluate a sequence of hypotheses about ensemble reasoning.
We begin by testing whether aggregation improves accuracy (H1), whether correct
answers tend to appear among alternative hypotheses (H2), and whether internal
consensus predicts correctness (H3). We then study the feasibility of selective
leader override as a function of internal consensus, and conclude with implications
and future directions. All claims are scoped to a single medical multiple-choice
QA benchmark, one model and prompt configuration, and a fixed sampling regime; we
do not claim generality beyond these conditions.

\section{Problem Setup and Definitions}

We study a medical QA benchmark where each prompt is evaluated via greedy decoding
and repeated sampling, yielding an empirical distribution over answer hypotheses
and associated explanations. We define the \emph{leader} answer as the most
frequent hypothesis in this distribution and quantify internal consensus using
the maximum agreement fraction across sampled branches.

Throughout this paper, we avoid the notion of symmetric or heuristic ``leader overrides'' of
predictions. Instead, we study \emph{selective leader override}: rejecting the
plurality (most frequent) hypothesis in favor of a lower-ranked alternative when
uncertainty indicators suggest the leader is unreliable.

\section{Experimental Setup}
\label{sec:setup}

We evaluate a single open-weight, instruction-tuned Llama 3.1 family checkpoint,
\href{https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-8B}
{\nolinkurl{HPAI-BSC/Llama3.1-Aloe-Beta-8B}}, which has 8B parameters and is run in full
precision (bfloat16, no quantization). The model is accessed via the Hugging Face
checkpoint and used with its chat template.

Our evaluation set is drawn from MedMCQA \parencite{pal2022medmcqa}
(\href{https://huggingface.co/datasets/openlifescienceai/medmcqa}
{\nolinkurl{openlifescienceai/medmcqa}}) using the official validation split. We
filter to single-choice questions with explanation length between 20 and 500
characters, then shuffle with seed 42 and select 400 questions while enforcing a
per-subject cap of 23 to prevent over-representation of subjects with very large
question pools (e.g., dentistry). MedMCQA contains 20 subject areas, and this
procedure yields a roughly balanced evaluation set across subjects. This yields
a deterministic 400-question evaluation set.

For each question, we generate:
\begin{itemize}
  \item a greedy prediction using deterministic decoding; and
  \item an ensemble of $N=10$ independently sampled reasoning paths.
\end{itemize}

Let $a_{i,j}$ denote the final answer of branch $j$ for question $i$. From the empirical distribution $P_i(a)$, we compute:
\begin{itemize}
  \item leader answer: $\arg\max_a P_i(a)$;
  \item maximum agreement fraction:
    \[
      \text{max\_frac}_i = \max_a P_i(a);
    \]
  \item Top-$k$ coverage:
    \[
      \text{Top-}k = \mathbb{1}\{y_i \in \text{Top-}k(P_i)\}.
    \]
\end{itemize}

All results are computed over this fixed 400-question evaluation set using a
single prompt and the model configuration above.

\section{Hypotheses and Empirical Tests}

\subsection*{Aggregation Improves Accuracy}

\textbf{H1.} Majority-vote aggregation improves accuracy relative to greedy decoding.

Let $\hat{y}_i^{(g)}$ denote the greedy prediction and $\hat{y}_i^{(m)}$ the majority prediction. Accuracy is defined as:
\[
\text{Acc} = \frac{1}{N}\sum_i \mathbb{1}[\hat{y}_i = y_i].
\]

Empirically:
\begin{itemize}
  \item Greedy accuracy: 65.75\%
  \item Majority accuracy: 66.75\%
\end{itemize}

A two-sided binomial test with null hypothesis $H_0: \pi = 0.6575$, where $\pi$ denotes the true success probability equal to the greedy decoding accuracy, yields a p-value of approximately $0.63$. We therefore fail to reject $H_0$, indicating that the observed difference between greedy and majority-vote accuracy is not statistically significant in this setting.

\paragraph{Interpretation.}
These results provide no statistical evidence in support of H1 in this experimental regime, suggesting that majority-vote aggregation does not reliably improve accuracy over greedy decoding under the tested conditions. While self-consistency can improve performance in some regimes \parencite{wang2022selfconsistency}, our results show that such gains are not evident in this single-model, single-dataset setting. Aggregation alone is therefore insufficient as a general reliability mechanism under these conditions.

\subsection*{Correct Answers Appear Among Alternatives}

\textbf{H2.} When the final prediction is incorrect, the correct answer is often present among alternative hypotheses.

We measure Top-2 coverage:
\[
\text{Top-2 coverage} = \frac{1}{N}\sum_i \mathbb{1}\{y_i \in \text{Top-2}(P_i)\}.
\]

Observed Top-2 coverage is 80.5\%, compared to a greedy accuracy of 65.75\%, corresponding to an absolute improvement of 14.75 percentage points. Using a binomial model with null hypothesis $H_0:\ \pi = 0.6575$ (where $\pi$ is the baseline success probability equal to the greedy accuracy), this difference is highly statistically significant ($p$-value $\ll 10^{-6}$).

\paragraph{Relation to prior work.}
This result is consistent with \textcite{kadavath2022know}, who show that models frequently possess internal signals of uncertainty. Our findings extend this by demonstrating that uncertainty manifests as \emph{explicit alternative hypotheses}, not merely as reduced confidence.

\subsection*{Internal Consensus Implies Correctness}
\label{sec:consensus}

\textbf{H3.} Strong internal agreement implies correctness.

We analyze unanimous cases where $\text{max\_frac}_i = 1.0$. Out of 400 questions:
\begin{itemize}
  \item Unanimous cases: 151
  \item Unanimous accuracy: 86.8\%
\end{itemize}

We test the null hypothesis $H_0: \pi \ge 0.95$, where $\pi$ denotes the true accuracy of unanimous predictions. A one-sided binomial test yields a p-value below $0.01$, allowing us to reject this hypothesis. Thus, even strong internal consensus does not guarantee near-perfect reliability.

\paragraph{Distributional analysis of consensus.}
Figure~\ref{fig:consensus} illustrates how accuracy varies with internal consensus. Accuracy increases monotonically with agreement but saturates well below perfect reliability. Notably, near-unanimous cases ($\text{max\_frac} \ge 0.9$) still exhibit error rates above 15\%.

\begin{figure}[H]
\centering
\IfFileExists{figure1_max_frac_exact.png}{%
  \includegraphics[width=0.45\textwidth]{figure1_max_frac_exact.png}%
}{%
  \fbox{\parbox[c][0.25\textheight][c]{0.45\textwidth}{\centering Missing figure: \texttt{figure1\_max\_frac\_exact.png}}}%
}
\captionsetup{width=0.6\textwidth}
\caption{
Accuracy as a function of internal consensus (max\_frac).
Higher branch agreement correlates with higher accuracy, but even near-unanimous predictions exhibit a non-zero error rate.
}\label{fig:consensus}
\end{figure}

This behavior is inconsistent with the assumption that confidence or agreement can serve as a sufficient decision criterion.

\paragraph{Key insight.}
Internal consensus reflects \emph{coherence}, not truth. Models can converge confidently on incorrect explanations, producing a dangerous illusion of reliability.

These results suggest that internal consensus is not only correlated with accuracy,
but may serve as a control signal for selective intervention.
These findings motivate evaluating whether selective leader override is practically feasible when restricted to low-consensus regimes.

\subsection*{Selective Leader Override Is Feasible}\label{sec:h4}

\textbf{H4.} Restricting leader override decisions to low-consensus regimes
substantially reduces the precision required of an override algorithm while
preserving access to a significant fraction of baseline errors.

\paragraph{Limits of naive top-2 leader override.}
A natural first idea is to apply leader override by switching predictions from the most frequent answer (top-1) to the second-most frequent answer (top-2) whenever uncertainty is detected. However, our results show that such naive leader override is generally unsafe. In regimes with strong consensus, correct top-1 predictions vastly outnumber correct top-2 predictions. As a result, even a small rate of incorrect leader overrides would outweigh any potential benefit. This imbalance is statistically significant in our data (binomial test, $p < 10^{-6}$ for high-consensus regimes), demonstrating that indiscriminate leader override would almost certainly degrade overall accuracy.

These findings indicate that top-2 coverage alone is insufficient: the feasibility of leader override depends critically on the relative frequencies of correct top-1 and top-2 hypotheses within a given uncertainty regime.

To make this trade-off explicit, we analyzed the feasibility of selective top-2 leader override across vote-consensus regimes defined by top-1 vote counts and top-1/top-2 gaps. For each regime, we quantified two key quantities (Figure~\ref{fig:flip-feasibility}).

First, we computed the maximum achievable overall accuracy under an idealized oracle that applies leader override in all cases where the second-most frequent hypothesis is correct (top-2 = gold) and never introduces new errors. This represents a strict upper bound, since errors outside top-2 coverage cannot be corrected by leader override.

Second, we measured the required false-override suppression, defined as the ratio between cases where the top-1 hypothesis is correct and cases where the top-2 hypothesis is correct within the regime. This ratio directly reflects how accurate a leader override decision policy must be in order to avoid degrading performance.

\section{Feasibility of Selective Leader Override}
\label{sec:feasibility}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure2_top2_flip_feasibility}
    \caption{
    Feasibility of selective top-2 leader override across vote-consensus regimes.
    The x-axis shows the maximum overall accuracy achievable by an ideal oracle that corrects all top-2=gold cases within a regime.
    The y-axis indicates the required false-override suppression (top-1 correct vs.\ top-2 correct).
    Bubble size reflects the number of baseline errors covered by the regime.
    }\label{fig:flip-feasibility}
\end{figure}

The analysis reveals a sharp, regime-dependent trade-off. Broad regimes that include high-consensus predictions offer larger theoretical gains but require extremely high leader override precision, making them practically infeasible. In contrast, more conservative regimes focused on high-uncertainty cases---where top-1 and top-2 receive comparable support---exhibit substantially more favorable error budgets. In these regimes, even moderately accurate leader override policies yield meaningful improvements while incurring limited risk.

While the fixed 400-question evaluation set limits statistical power for
fine-grained comparisons between neighboring regimes, the qualitative pattern
persists within this dataset and sampling configuration. In particular, excluding
high-consensus predictions (e.g., regimes with very high top-1 vote counts) leads
to a statistically significant reduction in the precision required of a leader
override algorithm, compared to regimes that include such cases.

We observe that moving the upper bound on top-1 vote counts from high-consensus regimes to uncertainty-focused regimes yields a more than twofold reduction in required false-override suppression (from $\approx 2.4$ to $\approx 1.0$), a difference that is statistically significant ($p < 10^{-6}$).

\section{Discussion}

The analysis presented in this work suggests that improving LLM performance in high-stakes medical settings is not primarily a matter of producing a single more accurate answer, but of reasoning over structured sets of competing hypotheses. In particular, the presence of meaningful top-2 coverage opens a path toward correcting errors, but only under carefully constrained decision regimes. These conclusions are limited to the single medical multiple-choice QA benchmark, model, prompt, and sampling settings tested here; broader generalization remains an open question.

These findings suggest a staged and risk-aware strategy for selective leader override. Rather than attempting to correct all errors, override algorithms should initially target regimes with high uncertainty and favorable error budgets, where the cost of a mistaken override is relatively low and the likelihood of successful correction is higher. As decision accuracy improves, such algorithms could progressively expand into more challenging regimes, but this should be evaluated in additional datasets and model families before drawing general claims.

Importantly, this perspective reframes leader override not as a binary operation applied globally, but as a selective intervention governed by explicit uncertainty thresholds and error budgets. Even conservative strategies restricted to high-uncertainty regimes can already produce substantial gains in this setting, without requiring near-perfect override decisions.

\subsection{Errors of Hypothesis Space}

The failure to find evidence supporting H3 motivates an alternative \emph{interpretive explanation} for these errors, consistent with the observed behavior but not yet empirically isolated as a distinct mechanism.

\paragraph{Interpretive hypothesis (H3$'$).}
One plausible interpretation is that some errors arise because the correct hypothesis is poorly represented or effectively absent from the model’s hypothesis space.

Under this view, unanimous but incorrect predictions correspond to cases where the model’s hypothesis space is locally coherent—yielding strong internal agreement—yet globally misaligned with the task. Importantly, the present analysis does not provide an operational test that cleanly separates missing hypotheses from low-probability selection among existing alternatives. We therefore do not claim to isolate this mechanism empirically.

Nevertheless, the observed behavior is inconsistent with an explanation based solely on stochastic selection noise. In these cases, increased sampling or stronger aggregation fails to recover the correct answer, suggesting a qualitative limitation of hypothesis representation rather than insufficient exploration.

This reframes aggregation not as a remedy for such errors, but as a \emph{diagnostic probe}: a means of revealing when internal coherence reflects confidence without correctness.

\paragraph{Operational breakdown of failure modes.}
To make the taxonomy measurable, we operationalize three error categories using the
$N{=}10$ sampled hypotheses per question. Among all evaluation items,
\textbf{Selection errors}---cases where the gold answer appears among the top-2 sampled
hypotheses but is not selected by the leader---account for
\textbf{[SEL\_TOTAL\_PCT]\%} of all questions and \textbf{[SEL\_ERROR\_PCT]\%} of leader
errors. In contrast, \textbf{unsurfaced errors under sampling}---cases where the gold
answer does not appear in the top-2 hypotheses---account for
\textbf{[UNS\_TOTAL\_PCT]\%} of all questions and \textbf{[UNS\_ERROR\_PCT]\%} of leader
errors. Finally, \textbf{high-consensus errors} highlight calibration limits: even in
unanimous regimes, \textbf{[UNAN\_WRONG\_PCT]\%} of predictions are incorrect
(\textbf{[UNAN\_WRONG\_N]} out of \textbf{[UNAN\_N]} unanimous cases), representing
\textbf{[UNAN\_SHARE\_ERRORS\_PCT]\%} of all leader errors. We treat the second category
as an \emph{operational proxy} for hypothesis-space limitations under this sampling
regime, without claiming mechanistic isolation.

Taken together, these results suggest three qualitatively distinct failure modes in LLM
reasoning:
\begin{enumerate}
  \item \textbf{Selection errors} (\textbf{[SEL\_ERROR\_PCT]\%} of errors): the correct
    hypothesis is present among alternatives (top-2) but not selected as the final
    answer.
  \item \textbf{Calibration/confidence errors} (e.g., \textbf{[UNAN\_WRONG\_PCT]\%} wrong
    even when unanimous): internal agreement is misaligned with correctness.
  \item \textbf{Hypothesis-space limitations} (proxied by
    \textbf{[UNS\_ERROR\_PCT]\%} of errors where gold is unsurfaced in top-2): the model
    converges on an internally coherent but incorrect explanation under the tested
    sampling regime.
\end{enumerate}

Importantly, only the first class is addressable through improved selection or
aggregation, while the latter two require changes to training objectives or hypothesis
representation.

\subsection{Relation to Prior Work}

\begin{itemize}
  \item \textbf{Self-consistency.} \textcite{wang2022selfconsistency} show that self-consistency can improve accuracy; we show its limits and failure modes in a medical QA setting.
  \item \textbf{Internal uncertainty.} \textcite{kadavath2022know} demonstrate internal uncertainty awareness; we show how this uncertainty appears as alternative hypotheses at the answer level.
  \item \textbf{Iterative refinement.} \textcite{madaan2023selfrefine} and STaR \parencite{zelikman2023star} focus on iterative correction and training; our work focuses on analysis rather than optimization.
  \item \textbf{Selective prediction / abstention.} Related questions arise in selective prediction and abstention for medical QA \parencite{machcha2025abstention}, where the objective is to know when \emph{not} to answer.
\end{itemize}

\section{Conclusion}

We evaluated a sequence of hypotheses about ensemble reasoning and selective
decision-making. In this setting, majority-vote aggregation did not yield a
statistically significant accuracy improvement over greedy decoding (H1), while
correct answers frequently appeared among alternative hypotheses (H2). Strong
internal consensus did not guarantee correctness (H3), motivating the interpretive
possibility of hypothesis-space errors (H3$'$), though we do not yet isolate that
mechanism empirically. We also found partial support for H4: selective leader
override appears feasible in low-consensus regimes with favorable error budgets,
but the evidence is limited to the single dataset, model, and sampling configuration
tested here. Together, these results underscore that benchmark accuracy can obscure qualitatively distinct failure modes that cannot be addressed by aggregation alone, calling for a distributional view of reasoning rather than reliance on a single trajectory.

\section{Future Work}

\begin{itemize}
  \item learning non-oracle override decision policies that operate within predefined error budgets;
  \item richer uncertainty signals and hypothesis-set representations beyond vote counts;
  \item scaling evaluations to larger models, datasets, and longer reasoning horizons; and
  \item generalization of override policies to other tasks and clinical domains.
\end{itemize}

\section*{Reproducibility and Code Availability}

All experiments reported in this paper are fully reproducible.
A public, versioned snapshot of the complete codebase is available at
\url{https://github.com/victorlavrenko/rofa/tree/paper/from-answers-to-hypotheses-v1}.

All results were obtained using the fixed model, dataset, prompt, and sampling
configuration described in
\hyperref[sec:setup]{Section~3}, evaluated on a deterministic 400-question subset
of the MedMCQA validation split.

Reproduction is supported by two dedicated notebooks:

\begin{itemize}
  \item Reproduction notebook (analysis only):\
  \href{https://github.com/victorlavrenko/rofa/blob/paper/from-answers-to-hypotheses-v1/notebooks/20_paper_reproduce.ipynb}{\texttt{20\_paper\_reproduce.ipynb}}
  {\small[\href{https://colab.research.google.com/github/victorlavrenko/rofa/blob/paper/from-answers-to-hypotheses-v1/notebooks/20_paper_reproduce.ipynb}{Colab~\extlink}]}

  \item Generation notebook (optional, stochastic):\
  \href{https://github.com/victorlavrenko/rofa/blob/paper/from-answers-to-hypotheses-v1/notebooks/10_colab_generate.ipynb}{\texttt{10\_colab\_generate.ipynb}}
  {\small[\href{https://colab.research.google.com/github/victorlavrenko/rofa/blob/paper/from-answers-to-hypotheses-v1/notebooks/10_colab_generate.ipynb}{Colab~\extlink}]}
\end{itemize}

\printbibliography[]

\end{document}
