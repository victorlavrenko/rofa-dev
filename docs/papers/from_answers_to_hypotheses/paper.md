# From Answers to Hypotheses: Internal Consensus and Its Limits (Extended Version)

## Abstract

Recent advances in large language models (LLMs) have demonstrated expert-level performance on medical question answering and diagnostic benchmarks, including recent near-clinical diagnostic systems and evaluations reported in 2024–2025, in some cases matching or exceeding average physician performance on structured tasks (Wang et al., 2022; Kadavath et al., 2022; Tu et al., 2025). At the same time, multiple studies have documented a persistent gap between benchmark accuracy and safe real‑world deployment, largely driven by miscalibrated confidence and coherent but incorrect reasoning—failure modes that are especially dangerous in high-stakes domains such as medicine.

Building on prior work on self‑consistency (Wang et al., 2022) and internal uncertainty estimation (Kadavath et al., 2022), this paper argues that the core limitation is not lack of knowledge, but a failure to properly surface and reason over the **distribution of internal hypotheses** generated by the model. Using controlled multi‑branch sampling on a medical QA benchmark, we empirically demonstrate three results: (i) majority‑vote aggregation does not yield statistically significant accuracy improvements over greedy decoding; (ii) the correct answer is frequently present among alternative hypotheses even when the final prediction is wrong; and (iii) strong internal consensus does *not* imply correctness.

We formalize these findings using explicit statistical tests, connect them to existing literature on calibration and self‑consistency, and propose an alternative failure mode: **errors of hypothesis space**, where the model converges confidently on a systematically wrong explanation. The results motivate a shift from answer‑centric evaluation toward hypothesis‑centric analysis, analogous to differential diagnosis in clinical medicine.

---

## 1. Introduction

Large language models have recently achieved impressive results on medical examinations and diagnostic benchmarks, in some cases rivaling or exceeding average physician performance. However, randomized studies and post‑deployment analyses suggest that these gains do not reliably translate into improved clinical decision‑making and may introduce new risks when model outputs are treated as definitive answers.

A dominant explanation attributes this gap to hallucinations and overconfidence. Yet prior work paints a more nuanced picture. **Kadavath et al. (2022)** show that language models often *know when they are likely to be wrong*, but this knowledge is not reliably exposed through standard decoding. Similarly, **Wang et al. (2022)** demonstrate that sampling multiple reasoning traces and aggregating them via self‑consistency can improve accuracy on some reasoning tasks.

Recent clinical and near-clinical evaluations further underscore the gap between benchmark performance and dependable decision support. In a randomized controlled study, access to an LLM did not significantly improve physicians’ diagnostic reasoning compared to conventional resources (Goh et al., 2024). At the same time, frontier systems such as AMIE demonstrate rapid progress toward conversational history-taking and differential diagnosis (Tu et al., 2025; McDuff et al., 2025), amplifying the importance of understanding when internally coherent model reasoning fails to correspond to correctness.

These findings suggest that uncertainty and alternative hypotheses already exist *inside* the model. The open question is whether these internal structures are sufficiently reliable to guide high‑stakes decisions. In medicine, uncertainty is rarely resolved by selecting a single hypothesis; instead, clinicians reason over *sets* of competing explanations. This work adopts that perspective and studies LLM reasoning as a **distribution over hypotheses**, rather than a single chain of thought.

Operationally, this distribution is observed through repeated sampling of the same prompt under fixed conditions, yielding an empirical distribution over final answers. We do not assume access to internal symbolic representations or latent states; rather, by “hypotheses” we refer to the set of discrete answer-level explanations implicitly instantiated by independent decoding trajectories, as opposed to explicit symbolic representations.

We emphasize that this work does not propose a new aggregation, verification, or decoding algorithm. Our goal is diagnostic rather than prescriptive: to characterize when and why aggregation and internal agreement fail, even under idealized sampling conditions.

---

## 2. Experimental Setup

For each question, we generate:

* A **greedy prediction** using deterministic decoding.
* An **ensemble** of (N=10) independently sampled reasoning paths.

Let (a_{i,j}) denote the final answer of branch (j) for question (i). From the empirical distribution (P_i(a)), we compute:

* **Leader answer**: ( \arg\max_a P_i(a) )
* **Maximum agreement fraction**:
  [
  \text{max_frac}_i = \max_a P_i(a)
  ]
* **Top‑k coverage**:
  [
  \text{Top‑}k = \mathbb{1}{y_i \in \text{Top‑}k(P_i)}
  ]

All results are computed over 400 questions using a fixed prompt and model configuration.

---

## 3. Hypothesis H1: Aggregation Improves Accuracy

**H1.** Majority‑vote aggregation improves accuracy relative to greedy decoding.

Let (\hat{y}_i^{(g)}) denote the greedy prediction and (\hat{y}_i^{(m)}) the majority prediction. Accuracy is defined as:
[
\text{Acc} = \frac{1}{N}\sum_i \mathbb{1}[\hat{y}_i = y_i]
]

Empirically:

* Greedy accuracy: **65.75%**
* Majority accuracy: **66.75%**

A two‑sided binomial test yields (p \approx 0.63), indicating that the observed difference is not statistically significant.

**Interpretation.** While self‑consistency can improve performance in some regimes (Wang et al., 2022), our results show that such gains are not robust in this setting. Aggregation alone is insufficient as a general reliability mechanism.

---

## 4. Hypothesis H2: Correct Answers Appear Among Alternatives

**H2.** When the final prediction is incorrect, the correct answer is often present among alternative hypotheses.

We measure **Top‑2 coverage**:
[
\text{Top‑2 coverage} = \frac{1}{N}\sum_i \mathbb{1}{y_i \in \text{Top‑2}(P_i)}
]

Observed Top-2 coverage is **80.5%**, compared to a greedy accuracy of **65.75%**, corresponding to an absolute improvement of **14.75 percentage points**. Under a binomial model with the null hypothesis H₀: p = 0.6575, this difference is highly statistically significant (p ≪ 10⁻⁶).

**Relation to prior work.** This result is consistent with Kadavath et al. (2022), who show that models frequently possess internal signals of uncertainty. Our findings extend this by demonstrating that uncertainty manifests as *explicit alternative hypotheses*, not merely as reduced confidence.

---

## 5. Hypothesis H3: Internal Consensus Implies Correctness

**H3.** Strong internal agreement implies correctness.

We analyze unanimous cases where (\text{max_frac}_i = 1.0). Out of 400 questions:

* Unanimous cases: 151
* Unanimous accuracy: **86.8%**

Testing against a conservative reliability threshold of 95% yields (p < 0.01), allowing us to reject H3.

**Key insight.** Internal consensus reflects *coherence*, not truth. Models can converge confidently on incorrect explanations, producing a dangerous illusion of reliability.

---

## 6. Distributional Analysis of Consensus

Figure 1 (not shown here) plots the empirical distribution of (\text{max_frac}). Accuracy increases monotonically with agreement but saturates well below perfect reliability. Near‑unanimous cases ((\text{max_frac} \ge 0.9)) still exhibit error rates above 15%.

This behavior is inconsistent with the assumption that confidence or agreement can serve as a sufficient decision criterion.

---

## 7. An Alternative Explanation: Errors of Hypothesis Space

The rejection of H3 motivates an alternative failure mode:

**H3′.** Some errors arise because the correct hypothesis is poorly represented or absent from the model’s hypothesis space.

Under this view, unanimous but incorrect predictions correspond to cases where the model’s hypothesis space is locally coherent but globally misaligned with the task. Such errors are not reducible to stochastic selection noise and therefore cannot be remedied by increased sampling or voting alone.

In such cases, increased sampling or stronger aggregation cannot recover the correct answer. This reframes aggregation as a *diagnostic probe* rather than a solution.

Taken together, our results suggest three qualitatively distinct failure modes in LLM reasoning:

1. **Selection errors**, where the correct hypothesis is present among alternatives but not selected as the final answer.
2. **Calibration or confidence errors**, where internal agreement or confidence is misaligned with correctness.
3. **Hypothesis space errors**, where the model converges on an internally coherent but systematically incorrect explanation.

Importantly, only the first class is addressable through improved selection or aggregation, while the latter two require changes to training objectives or hypothesis representation.

---

## 8. Relation to Prior Work

* **Wang et al. (2022)** show that self‑consistency can improve accuracy; we show its limits and failure modes.
* **Kadavath et al. (2022)** demonstrate internal uncertainty awareness; we show how this uncertainty appears as alternative hypotheses.
* **Self‑Refine and STaR** focus on iterative correction and training; our work focuses on *analysis* rather than optimization.

---

## 9. Implications and Future Work

These results suggest that safe deployment of LLMs in medicine requires decision rules that operate on *sets of hypotheses*, not single answers. Future work should focus on:

* Explicit modeling of hypothesis spaces
* Training objectives that penalize confident convergence on incorrect hypotheses
* Interfaces that expose structured uncertainty to clinicians

---

## References

Wang, X., Wei, J., Schuurmans, D., et al. (2022).  
*Self-Consistency Improves Chain of Thought Reasoning in Language Models*.  
arXiv:2203.11171.

Kadavath, S., et al. (2022).  
*Language Models (Mostly) Know What They Know*.  
arXiv:2207.05221.

Madaan, A., et al. (2023).  
*Self-Refine: Iterative Refinement with Self-Feedback*.  
arXiv:2303.17651.

Zelikman, E., et al. (2023).  
*STaR: Bootstrapping Reasoning With Reasoning*.  
arXiv:2203.14465.

Goh, K. H., et al. (2024).  
*Randomized Evaluation of Large Language Models for Clinical Diagnostic Reasoning*.  
JAMA Network Open.

Tu, T., et al. (2025).  
*Towards Conversational Diagnostic Artificial Intelligence (AMIE)*.  
Nature.

McDuff, D., et al. (2025).  
*Towards Accurate Differential Diagnosis with Large Language Models*.  
Nature Medicine.

Machcha, V., et al. (2025).  
*Do Large Language Models Know When Not to Answer? Selective Prediction and Abstention in Medical Question Answering*.  
Proceedings of the ACL Workshop on Uncertainty in Natural Language Processing.
