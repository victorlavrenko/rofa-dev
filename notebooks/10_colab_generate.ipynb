{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 0 \u2014 Bootstrap\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "OUT_BASE = \"/content/drive/MyDrive/rofa_runs\"\n",
        "!git clone https://github.com/victorlavrenko/rofa\n",
        "%cd rofa\n",
        "%pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 1 \u2014 Validate environment (Drive + output + model)\n",
        "from pathlib import Path\n",
        "\n",
        "from rofa.core.model import MODEL_ID, load_model_with_fallback, load_tokenizer\n",
        "\n",
        "out_base = Path(OUT_BASE)\n",
        "out_base.mkdir(parents=True, exist_ok=True)\n",
        "assert out_base.exists() and out_base.is_dir(), \"Output base not available\"\n",
        "(out_base / \"tmp_write_check.txt\").write_text(\"ok\")\n",
        "(out_base / \"tmp_write_check.txt\").unlink()\n",
        "\n",
        "tokenizer = load_tokenizer()\n",
        "model = load_model_with_fallback()\n",
        "print(f\"Model ready: {MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 2 \u2014 Create a fixed question set (IDs)\n",
        "from pathlib import Path\n",
        "\n",
        "from rofa.core.question_set import create_question_set, save_question_set\n",
        "\n",
        "DATASET_NAME = \"openlifescienceai/medmcqa\"\n",
        "DATASET_SPLIT = \"validation\"\n",
        "SEED = 42\n",
        "N = 100\n",
        "SUBJECTS = 20\n",
        "\n",
        "question_set = create_question_set(\n",
        "    {\"dataset_name\": DATASET_NAME, \"dataset_split\": DATASET_SPLIT},\n",
        "    {\n",
        "        \"seed\": SEED,\n",
        "        \"n\": N,\n",
        "        \"subjects\": SUBJECTS,\n",
        "        \"max_per_subject\": N / SUBJECTS * 1.1 + 1,\n",
        "    },\n",
        ")\n",
        "\n",
        "qs_dir = Path(OUT_BASE) / \"question_sets\"\n",
        "qs_dir.mkdir(parents=True, exist_ok=True)\n",
        "question_set_id = question_set.qs_id\n",
        "qs_path = qs_dir / f\"{question_set_id}.json\"\n",
        "save_question_set(question_set, str(qs_path))\n",
        "\n",
        "print(f\"Saved question set: {question_set_id} -> {qs_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 3 \u2014 Run greedy generation (native Python call)\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from rofa.core.runner import run_generation\n",
        "from rofa.core.schemas import GenerationConfig\n",
        "from rofa.papers.from_answers_to_hypotheses.methods import GreedyDecode\n",
        "\n",
        "run_id = f\"greedy_{question_set_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "run_dir = Path(OUT_BASE) / \"runs\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "config = GenerationConfig(\n",
        "    method=\"greedy\",\n",
        "    model_id=MODEL_ID,\n",
        "    out_dir=str(run_dir),\n",
        "    run_id=run_id,\n",
        "    seed=SEED,\n",
        "    max_new_tokens=1024,\n",
        "    n=N,\n",
        "    subjects=SUBJECTS,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    dataset_split=DATASET_SPLIT,\n",
        "    question_set_path=str(qs_path),\n",
        "    progress=True,\n",
        "    heartbeat_every=10,\n",
        "    write_full_records=False,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    method_impl=GreedyDecode(),\n",
        ")\n",
        "\n",
        "run_generation(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 4 \u2014 Run k-sample ensemble generation (branches alias)\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from rofa.core.runner import run_generation\n",
        "from rofa.core.schemas import GenerationConfig\n",
        "from rofa.papers.from_answers_to_hypotheses.methods import BranchSamplingEnsemble\n",
        "\n",
        "run_id = f\"k_sample_ensemble_{question_set_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "run_dir = Path(OUT_BASE) / \"runs\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "config = GenerationConfig(\n",
        "    method=\"k_sample_ensemble\",\n",
        "    model_id=MODEL_ID,\n",
        "    out_dir=str(run_dir),\n",
        "    run_id=run_id,\n",
        "    seed=SEED,\n",
        "    max_new_tokens=1024,\n",
        "    n=N,\n",
        "    subjects=SUBJECTS,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    dataset_split=DATASET_SPLIT,\n",
        "    question_set_path=str(qs_path),\n",
        "    n_branches=10,\n",
        "    temperature=0.8,\n",
        "    top_p=0.8,\n",
        "    top_k=50,\n",
        "    progress=True,\n",
        "    heartbeat_every=10,\n",
        "    write_full_records=True,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    method_impl=BranchSamplingEnsemble(n_branches=10, temperature=0.8, top_p=0.8, top_k=50),\n",
        ")\n",
        "\n",
        "run_generation(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Publish your run artifacts to GitHub Releases (manual)\n",
        "\n",
        "1. Open Google Drive and locate your run folder under `OUT_BASE/runs/<run_id>/`.\n",
        "2. Download the run folder as a `.zip`.\n",
        "3. Create a new GitHub Release in your repository.\n",
        "4. Upload the `.zip` as a release asset.\n",
        "5. Paste the asset URL into the analysis notebook so it can download the artifacts.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}