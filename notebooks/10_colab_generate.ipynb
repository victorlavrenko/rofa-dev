{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0 \u2014 Bootstrap\n",
        "!git clone https://github.com/victorlavrenko/rofa\n",
        "%cd rofa\n",
        "%pip install -e .\n",
        "# If editable install fails, fallback to PYTHONPATH:\n",
        "# import sys\n",
        "# sys.path.append('/content/rofa')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "OUT_BASE = '/content/drive/MyDrive/rofa_runs'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1 \u2014 Prepare the fixed question set (IDs)\n",
        "from pathlib import Path\n",
        "\n",
        "from rofa.question_set import create_question_set, save_question_set\n",
        "\n",
        "DATASET_NAME = 'openlifescienceai/medmcqa'\n",
        "DATASET_SPLIT = 'validation'\n",
        "SEED = 42\n",
        "N = 200\n",
        "SUBJECTS = 20\n",
        "\n",
        "qs = create_question_set(\n",
        "    {'dataset_name': DATASET_NAME, 'dataset_split': DATASET_SPLIT},\n",
        "    {\n",
        "        'seed': SEED,\n",
        "        'n': N,\n",
        "        'subjects': SUBJECTS,\n",
        "        'max_per_subject': N / SUBJECTS * 1.1 + 1,\n",
        "    },\n",
        ")\n",
        "\n",
        "qs_dir = Path(OUT_BASE) / 'question_sets'\n",
        "qs_dir.mkdir(parents=True, exist_ok=True)\n",
        "qs_path = qs_dir / f'{qs.qs_id}.json'\n",
        "save_question_set(qs, str(qs_path))\n",
        "\n",
        "qs_id = qs.qs_id\n",
        "print(f'Saved question set: {qs_id} -> {qs_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2 \u2014 Run Greedy generation (native Python call)\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from rofa.methods import GreedyDecode\n",
        "from rofa.model import MODEL_ID, load_model_with_fallback, load_tokenizer\n",
        "from rofa.runner import run_generation\n",
        "from rofa.schemas import GenerationConfig\n",
        "\n",
        "RUN_ID_GREEDY = f\"greedy_{qs_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUNS_DIR = Path(OUT_BASE) / 'runs'\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tokenizer = load_tokenizer()\n",
        "model = load_model_with_fallback()\n",
        "\n",
        "config = GenerationConfig(\n",
        "    method='greedy',\n",
        "    model_id=MODEL_ID,\n",
        "    out_dir=str(RUNS_DIR),\n",
        "    run_id=RUN_ID_GREEDY,\n",
        "    seed=SEED,\n",
        "    max_new_tokens=1024,\n",
        "    n=N,\n",
        "    subjects=SUBJECTS,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    dataset_split=DATASET_SPLIT,\n",
        "    question_set_path=str(qs_path),\n",
        "    progress=True,\n",
        "    heartbeat_every=10,\n",
        "    write_full_records=False,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    method_impl=GreedyDecode(),\n",
        ")\n",
        "\n",
        "run_generation(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3 \u2014 Run Branches generation (native Python call)\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from rofa.methods import BranchSamplingEnsemble\n",
        "from rofa.model import MODEL_ID, load_model_with_fallback, load_tokenizer\n",
        "from rofa.runner import run_generation\n",
        "from rofa.schemas import GenerationConfig\n",
        "\n",
        "RUN_ID_BRANCHES = f\"branches_{qs_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUNS_DIR = Path(OUT_BASE) / 'runs'\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tokenizer = load_tokenizer()\n",
        "model = load_model_with_fallback()\n",
        "\n",
        "config = GenerationConfig(\n",
        "    method='branches',\n",
        "    model_id=MODEL_ID,\n",
        "    out_dir=str(RUNS_DIR),\n",
        "    run_id=RUN_ID_BRANCHES,\n",
        "    seed=SEED,\n",
        "    max_new_tokens=1024,\n",
        "    n=N,\n",
        "    subjects=SUBJECTS,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    dataset_split=DATASET_SPLIT,\n",
        "    question_set_path=str(qs_path),\n",
        "    n_branches=10,\n",
        "    temperature=0.8,\n",
        "    top_p=0.8,\n",
        "    top_k=50,\n",
        "    progress=True,\n",
        "    heartbeat_every=10,\n",
        "    write_full_records=True,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    method_impl=BranchSamplingEnsemble(n_branches=10, temperature=0.8, top_p=0.8, top_k=50),\n",
        ")\n",
        "\n",
        "run_generation(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Publish your run artifacts to GitHub Releases (manual)\n",
        "\n",
        "1. Open Google Drive and locate your run folder under `OUT_BASE/runs/<run_id>/`.\n",
        "2. Download the run folder as a `.zip`.\n",
        "3. Create a new GitHub Release in your repository.\n",
        "4. Upload the `.zip` as a release asset.\n",
        "5. Paste the asset URL into the analysis notebook so it can download the artifacts.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}