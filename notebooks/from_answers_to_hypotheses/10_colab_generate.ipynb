{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROFA \u2014 Data Generation Notebook\n",
    "\n",
    "This notebook runs the **data generation stage** for the paper  \n",
    "*From Answers to Hypotheses: Internal Consensus and Its Limits in Large Language Models*.\n",
    "\n",
    "It executes model inference under fixed decoding settings and produces\n",
    "versioned run artifacts (JSON/JSONL) that capture:\n",
    "- per-question model outputs,\n",
    "- alternative sampled hypotheses,\n",
    "- metadata required for downstream analysis.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The outputs of this notebook are **inputs** to the analysis and reproduction\n",
    "pipeline implemented in `20_paper_reproduce.ipynb`.\n",
    "They are not required for reproducing the paper figures if you use the\n",
    "pre-generated release artifacts.\n",
    "\n",
    "## Usage modes\n",
    "\n",
    "- **Reproduce paper results (recommended):**  \n",
    "  Skip this notebook and download the released run artifacts from GitHub.\n",
    "\n",
    "- **Regenerate data (optional):**  \n",
    "  Run this notebook to regenerate model outputs, e.g. to:\n",
    "  - test alternative decoding parameters,\n",
    "  - evaluate new models,\n",
    "  - extend experiments beyond the paper.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Generation can be time- and compute-intensive.\n",
    "- Results depend on model checkpoints, decoding parameters, and random seeds.\n",
    "- This notebook is typically executed in Colab or a GPU-enabled environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"HPAI-BSC/Llama3.1-Aloe-Beta-8B\"\n",
    "# Recommended medical models (examples only):\n",
    "# MODEL_ID = \"HPAI-BSC/Llama3.1-Aloe-Beta-8B\"\n",
    "# MODEL_ID = \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\"\n",
    "# MODEL_ID = \"m42-health/Llama3-Med42-8B\"\n",
    "# MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
    "# MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "DATASET_NAME = \"openlifescienceai/medmcqa\"\n",
    "DATASET_SPLIT = \"validation\"\n",
    "SEED = 42\n",
    "N = 400\n",
    "SUBJECTS = 20\n",
    "MAX_NEW_TOKENS = 1024\n",
    "\n",
    "# Branch sampling settings.\n",
    "N_BRANCHES = 10\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.8\n",
    "TOP_K = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install ROFA package if not already installed\n",
    "import importlib.metadata\n",
    "\n",
    "try: \n",
    "  importlib.metadata.distribution(\"rofa\")\n",
    "except importlib.metadata.PackageNotFoundError: \n",
    "  from pathlib import Path\n",
    "  if (Path.cwd().parent.parent / \"pyproject.toml\").is_file():\n",
    "      %pip install -e \"../..\"\n",
    "  else:\n",
    "      if not Path(\"rofa\").is_dir():\n",
    "          !git clone https://github.com/victorlavrenko/rofa\n",
    "      %pip install -e \"rofa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \u2014 Validate environment (Drive + output + model)\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"rofa.core\") is None:\n",
    "    print(\n",
    "        \"\\n\u26a0\ufe0f  Runtime restart required\\n\\n\"\n",
    "        \"ROFA has just been installed, but the Python runtime has not been restarted yet.\\n\\n\"\n",
    "        \"Please restart the runtime via:\\n\"\n",
    "        \"  Runtime (or \u25bc after Run all) \u2192 Restart runtime (or Restart runtime and run all)\\n\"\n",
    "        \"This is expected behaviour in Google Colab.\"\n",
    "    )\n",
    "    raise SystemExit\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from rofa.core.model import load_model_with_fallback, load_tokenizer\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    if not os.path.isdir(\"/content/drive\"):\n",
    "        drive.mount(\"/content/drive\")\n",
    "    OUT_BASE = \"/content/drive/MyDrive/rofa_runs\"\n",
    "except ImportError:\n",
    "    OUT_BASE = \"./rofa_runs\"\n",
    "\n",
    "out_base = Path(OUT_BASE)\n",
    "out_base.mkdir(parents=True, exist_ok=True)\n",
    "assert out_base.exists() and out_base.is_dir(), \"Output base not available\"\n",
    "(out_base / \"tmp_write_check.txt\").write_text(\"ok\")\n",
    "(out_base / \"tmp_write_check.txt\").unlink()\n",
    "\n",
    "tokenizer = load_tokenizer(MODEL_ID)\n",
    "model = load_model_with_fallback(MODEL_ID)\n",
    "print(f\"Model ready: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1b \u2014 Model preflight\n",
    "from rofa.core.model_id import to_slug\n",
    "\n",
    "model_slug = to_slug(MODEL_ID)\n",
    "print(f\"Selected MODEL_ID: {MODEL_ID}\")\n",
    "print(f\"Model slug: {model_slug}\")\n",
    "\n",
    "# Gated-model checklist (e.g., google/medgemma-1.5-4b-it):\n",
    "# 1) Open the model page on Hugging Face while logged in.\n",
    "# 2) Click Agree/Access to accept terms (if required).\n",
    "# 3) Create a Hugging Face access token (read scope).\n",
    "# 4) Authenticate here (paste token manually or read from env):\n",
    "#    from huggingface_hub import login\n",
    "#    login(token=\"...\")\n",
    "# 5) Alternatively: os.environ[\"HF_TOKEN\"] = \"...\"\n",
    "# Never hardcode secrets into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 \u2014 Create a fixed question set (IDs)\n",
    "from pathlib import Path\n",
    "\n",
    "from rofa.core.question_set import create_question_set, save_question_set\n",
    "\n",
    "question_set = create_question_set(\n",
    "    {\"dataset_name\": DATASET_NAME, \"dataset_split\": DATASET_SPLIT},\n",
    "    {\n",
    "        \"seed\": SEED,\n",
    "        \"n\": N,\n",
    "        \"subjects\": SUBJECTS,\n",
    "        \"max_per_subject\": N / SUBJECTS * 1.1 + 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "qs_dir = Path(OUT_BASE) / \"question_sets\"\n",
    "qs_dir.mkdir(parents=True, exist_ok=True)\n",
    "question_set_id = question_set.qs_id\n",
    "qs_path = qs_dir / f\"{question_set_id}.json\"\n",
    "save_question_set(question_set, str(qs_path))\n",
    "\n",
    "print(f\"Saved question set: {question_set_id} -> {qs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 \u2014 Run generation (auto resume/expand/new)\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "from rofa.core.io import load_manifest, load_progress\n",
    "from rofa.core.runner import run_generation\n",
    "from rofa.core.schemas import GenerationConfig\n",
    "from rofa.papers.from_answers_to_hypotheses.methods import (\n",
    "    BranchSamplingEnsemble,\n",
    "    GreedyDecode,\n",
    ")\n",
    "\n",
    "run_root = Path(OUT_BASE) / \"runs\" / \"from_answers_to_hypotheses\" / model_slug\n",
    "run_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _default_run_id(prefix: str) -> str:\n",
    "    timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}_{question_set_id}_{timestamp}\"\n",
    "\n",
    "def _dir_mtime(path: Path) -> float:\n",
    "    latest = path.stat().st_mtime\n",
    "    for root, _, files in os.walk(path):\n",
    "        for name in files:\n",
    "            candidate = Path(root) / name\n",
    "            try:\n",
    "                latest = max(latest, candidate.stat().st_mtime)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    return latest\n",
    "\n",
    "def _question_total(run_dir: Path) -> int | None:\n",
    "    qs_path = run_dir / \"question_set.json\"\n",
    "    if not qs_path.exists():\n",
    "        return None\n",
    "    with qs_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        payload = json.load(f)\n",
    "    return len(payload.get(\"examples\", []))\n",
    "\n",
    "def _count_lines(path: Path) -> int | None:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def _latest_run(method: str):\n",
    "    candidates = []\n",
    "    for child in run_root.iterdir():\n",
    "        if not child.is_dir():\n",
    "            continue\n",
    "        manifest = load_manifest(str(child / \"manifest.json\"))\n",
    "        if manifest is None or manifest.method != method:\n",
    "            continue\n",
    "        candidates.append((child, _dir_mtime(child)))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    run_dir, _ = max(candidates, key=lambda item: item[1])\n",
    "    total = _question_total(run_dir)\n",
    "    progress = load_progress(str(run_dir / \"progress.json\"))\n",
    "    summary_count = _count_lines(run_dir / \"summary.jsonl\")\n",
    "    position = progress.get(\"position\") if progress else None\n",
    "    complete = False\n",
    "    if total is not None:\n",
    "        if position is not None:\n",
    "            complete = position >= total\n",
    "        elif summary_count is not None:\n",
    "            complete = summary_count >= total\n",
    "    return {\n",
    "        \"run_id\": run_dir.name,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"total\": total,\n",
    "        \"complete\": complete,\n",
    "    }\n",
    "\n",
    "def _build_config(method: str, run_id: str, *, resume: bool | None, expand: bool, n: int):\n",
    "    if method == \"greedy\":\n",
    "        method_impl = GreedyDecode()\n",
    "        write_full_records = False\n",
    "    else:\n",
    "        method_impl = BranchSamplingEnsemble(\n",
    "            n_branches=N_BRANCHES,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "        )\n",
    "        write_full_records = True\n",
    "    return GenerationConfig(\n",
    "        method=method,\n",
    "        model_id=MODEL_ID,\n",
    "        model_slug=model_slug,\n",
    "        out_dir=str(run_root),\n",
    "        run_id=run_id,\n",
    "        resume=resume,\n",
    "        expand=expand,\n",
    "        seed=SEED,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        n=n,\n",
    "        subjects=SUBJECTS,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        dataset_split=DATASET_SPLIT,\n",
    "        question_set_path=str(qs_path),\n",
    "        n_branches=N_BRANCHES if method == \"k_sample_ensemble\" else None,\n",
    "        temperature=TEMPERATURE if method == \"k_sample_ensemble\" else None,\n",
    "        top_p=TOP_P if method == \"k_sample_ensemble\" else None,\n",
    "        top_k=TOP_K if method == \"k_sample_ensemble\" else None,\n",
    "        progress=True,\n",
    "        write_full_records=write_full_records,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        method_impl=method_impl,\n",
    "    )\n",
    "\n",
    "def _schedule_new_runs():\n",
    "    runs = []\n",
    "    runs.append((\"greedy\", _default_run_id(\"greedy\"), None, False, N))\n",
    "    runs.append((\"k_sample_ensemble\", _default_run_id(\"k_sample_ensemble\"), None, False, N))\n",
    "    return runs\n",
    "\n",
    "greedy = _latest_run(\"greedy\")\n",
    "branches = _latest_run(\"k_sample_ensemble\")\n",
    "planned_runs = []\n",
    "\n",
    "if greedy and not greedy[\"complete\"]:\n",
    "    planned_runs.append((\"greedy\", greedy[\"run_id\"], True, False, N))\n",
    "    if branches and not branches[\"complete\"]:\n",
    "        planned_runs.append((\"k_sample_ensemble\", branches[\"run_id\"], True, False, N))\n",
    "    elif branches is None:\n",
    "        planned_runs.append((\"k_sample_ensemble\", _default_run_id(\"k_sample_ensemble\"), None, False, N))\n",
    "elif greedy and greedy[\"complete\"]:\n",
    "    if not branches or not branches[\"complete\"]:\n",
    "        run_id = branches[\"run_id\"] if branches else _default_run_id(\"k_sample_ensemble\")\n",
    "        resume = True if branches else None\n",
    "        planned_runs.append((\"k_sample_ensemble\", run_id, resume, False, N))\n",
    "    else:\n",
    "        greedy_total = greedy[\"total\"]\n",
    "        branches_total = branches[\"total\"]\n",
    "        if greedy_total and branches_total and greedy_total < N and branches_total < N:\n",
    "            planned_runs.append((\"greedy\", greedy[\"run_id\"], True, True, N))\n",
    "            planned_runs.append((\"k_sample_ensemble\", branches[\"run_id\"], True, True, N))\n",
    "        else:\n",
    "            planned_runs.extend(_schedule_new_runs())\n",
    "else:\n",
    "    planned_runs.extend(_schedule_new_runs())\n",
    "\n",
    "print(\"Planned runs:\")\n",
    "for method, run_id, resume, expand, n in planned_runs:\n",
    "    print(f\"- {method} | run_id={run_id} | resume={resume} | expand={expand} | n={n}\")\n",
    "\n",
    "for method, run_id, resume, expand, n in planned_runs:\n",
    "    config = _build_config(method, run_id, resume=resume, expand=expand, n=n)\n",
    "    run_generation(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Continue or expand a run\n",
    "\n",
    "This notebook now auto-detects the most recent greedy/branches runs under `OUT_BASE`\n",
    "and decides whether to resume, expand, or start fresh. Update the config values above\n",
    "(like `N`, `N_BRANCHES`, or `MAX_NEW_TOKENS`) and rerun the generation cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Publish your run artifacts to GitHub Releases (manual)\n",
    "\n",
    "1. Open Google Drive and locate your run folder under `OUT_BASE/runs/<run_id>/`.\n",
    "2. Download the run folder as a `.zip`.\n",
    "3. Create a new GitHub Release in your repository.\n",
    "4. Upload the `.zip` as a release asset.\n",
    "5. Paste the asset URL into the analysis notebook so it can download the artifacts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}