{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Reproducing \"From Answers to Hypotheses\"\n",
    "\n",
    "This notebook reproduces all figures and statistical analyses reported in:\n",
    "\n",
    "> Victor Lavrenko. *From Answers to Hypotheses: Internal Consensus and Its Limits in Large Language Models*. 2026.\n",
    "\n",
    "The results correspond to the tagged release:\n",
    "`paper/from-answers-to-hypotheses-v1`\n",
    "\n",
    "Repository:\n",
    "https://github.com/victorlavrenko/rofa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the ROFA package and load the precomputed runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "# install ROFA package if not already installed\n",
    "import importlib.metadata\n",
    "\n",
    "try: \n",
    "  importlib.metadata.distribution(\"rofa\")\n",
    "except importlib.metadata.PackageNotFoundError: \n",
    "  from pathlib import Path\n",
    "  if (Path.cwd().parent.parent / \"pyproject.toml\").is_file():\n",
    "      %pip install -e \"../..\"\n",
    "  else:\n",
    "      if not Path(\"rofa\").is_dir():\n",
    "          !git clone https://github.com/victorlavrenko/rofa\n",
    "      %pip install -e \"rofa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"rofa.papers\") is None:\n",
    "    print(\n",
    "        \"\\n⚠️  Runtime restart required\\n\\n\"\n",
    "        \"ROFA has just been installed, but the Python runtime has not been restarted yet.\\n\\n\"\n",
    "        \"Please restart the runtime via:\\n\"\n",
    "        \"  Runtime (or ▼ after Run all) → Restart runtime (or Restart runtime and run all)\\n\"\n",
    "        \"This is expected behaviour in Google Colab.\"\n",
    "    )\n",
    "    raise SystemExit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rofa.papers.from_answers_to_hypotheses import analysis, notebook_helpers\n",
    "\n",
    "# Get run artifacts\n",
    "run_dir_greedy, greedy_asset_url = (\n",
    "    r\"\",\n",
    "    \"https://github.com/victorlavrenko/rofa/releases/download/paper%2Ffrom-answers-to-hypotheses-v1/rofa-from-answers-to-hypotheses-runs-v1-greedy.zip\",\n",
    ")\n",
    "run_dir_k_sample, k_sample_asset_url = (\n",
    "    r\"\",\n",
    "    \"https://github.com/victorlavrenko/rofa/releases/download/paper%2Ffrom-answers-to-hypotheses-v1/rofa-from-answers-to-hypotheses-runs-v1-branches10.zip\",\n",
    ")\n",
    "run_inputs = notebook_helpers.resolve_run_inputs(\n",
    "    run_dir_greedy, greedy_asset_url, run_dir_k_sample, k_sample_asset_url\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "a135c5f2-3bae-4864-d717-3ba79fba7c28"
   },
   "outputs": [],
   "source": [
    "# Load + validate\n",
    "df_greedy, df_branches, metadata = analysis.load_paper_runs(run_inputs)\n",
    "notebook_helpers.validate_required_columns(df_greedy, df_branches)\n",
    "notebook_helpers.print_run_summary(df_greedy, df_branches, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## H1: Aggregation Improves Accuracy\n",
    "\n",
    "> Greedy accuracy: 65.75%\n",
    "> Majority accuracy: 66.75%\n",
    "> A two-sided binomial test with null hypothesis $H_0: \\pi = 0.6575$ yields a p-value of approximately 0.63, indicating no statistically significant difference between greedy and majority-vote accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "# R1: greedy accuracy\n",
    "df_greedy_accuracy = pd.DataFrame(\n",
    "    {\"metric\": [\"greedy_accuracy\"], \"value\": [analysis.accuracy_greedy(df_greedy)]}\n",
    ")\n",
    "df_greedy_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "# R2: leader accuracy\n",
    "df_leader_accuracy = pd.DataFrame(\n",
    "    {\"metric\": [\"leader_accuracy\"], \"value\": [analysis.accuracy_leader(df_branches)]}\n",
    ")\n",
    "df_leader_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "# R9: majority vote does not help (greedy vs leader)\n",
    "df_majority_vote = notebook_helpers.majority_vote_table(df_greedy, df_branches)\n",
    "df_majority_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## H2: Correct Answers Appear Among Alternatives\n",
    "\n",
    "> Observed Top-2 coverage is 80.5%, compared to a greedy accuracy of 65.75%, corresponding to an absolute improvement of 14.75 percentage points. Using a binomial model with null hypothesis $H_0: \\pi = 0.6575$, this difference is highly statistically significant ($p$-value $\\ll 10^{-6}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "# R6: top-2 coverage\n",
    "df_top2 = analysis.compute_table_top2(df_branches)\n",
    "df_top2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## H3: Internal Consensus Implies Correctness\n",
    "\n",
    "> Unanimous cases: 151\n",
    "> Unanimous accuracy: 86.8%\n",
    "> A one-sided binomial test of $H_0: \\pi \\ge 0.95$ yields a p-value below 0.01, so even strong internal consensus does not guarantee near-perfect reliability.\n",
    "> Near-unanimous cases ($\\text{max\\_frac} \\ge 0.9$) still exhibit error rates above 15%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "# R4: unanimous stats\n",
    "unanimous_stats = analysis.unanimous_stats(df_branches)\n",
    "df_unanimous = pd.DataFrame([unanimous_stats])\n",
    "df_unanimous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "# R5: near-unanimous stats\n",
    "near_unanimous_stats = analysis.near_unanimous_stats(df_branches, threshold=0.9)\n",
    "df_near_unanimous = pd.DataFrame([near_unanimous_stats])\n",
    "df_near_unanimous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R5b: operational failure-mode breakdown (top-2 + unanimity)\n",
    "failure_mode_stats = analysis.failure_mode_breakdown(\n",
    "    df_branches, top_k=2, near_threshold=0.9\n",
    ")\n",
    "df_failure_modes = notebook_helpers.failure_mode_table(failure_mode_stats)\n",
    "df_failure_modes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "> Operational breakdown of failure modes. To make the taxonomy measurable, we operationalize three\n",
    "> error categories using the $N=10$ sampled hypotheses per question. Among all evaluation items, selection\n",
    "> errors—cases where the gold answer appears among the top-2 sampled hypotheses but is not selected\n",
    "> by the leader—account for 13.8% of all questions and 41.4% of leader errors. In contrast, unsurfaced\n",
    "> errors under sampling—cases where the gold answer does not appear in the top-2 hypotheses—account\n",
    "> for 19.5% of all questions and 58.6% of leader errors. Finally, high-consensus errors highlight calibration\n",
    "> limits: even in unanimous regimes, 13.2% of predictions are incorrect (20 out of 151 unanimous cases),\n",
    "> representing 15.0% of all leader errors. We treat the second category as an operational proxy for\n",
    "> hypothesis-space limitations under this sampling regime, without claiming mechanistic isolation. All\n",
    "> percentages are reported as descriptive statistics for this fixed evaluation set and should be interpreted as\n",
    "> approximate rather than precise estimates.\n",
    ">\n",
    "> Taken together, these results suggest three qualitatively distinct failure modes in LLM reasoning:\n",
    "> 1. Selection errors (41.4% of errors): the correct hypothesis is present among alternatives (top-2) but\n",
    "> not selected as the final answer.\n",
    "> 2. Calibration/confidence errors (e.g., 13.2% wrong even when unanimous): internal agreement is\n",
    "> misaligned with correctness.\n",
    "> 3. Hypothesis-space limitations (proxied by 58.6% of errors where the gold answer is unsurfaced in\n",
    "> the top-2): the model converges on an internally coherent but incorrect explanation under the tested\n",
    "> sampling regime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "> Figure 1: Accuracy as a function of internal consensus (max_frac). Higher branch agreement correlates with higher accuracy, but even near-unanimous predictions exhibit a non-zero error rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "7"
   },
   "outputs": [],
   "source": [
    "# Figure 1: accuracy vs internal consensus (max_frac_exact)\n",
    "df_max_frac_exact = notebook_helpers.plot_accuracy_vs_consensus(\n",
    "    df_branches, \"figure1_max_frac_exact.png\"\n",
    ")\n",
    "df_max_frac_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## H4: Selective Leader Override Is Feasible\n",
    "\n",
    "> We analyzed selective top-2 leader override across vote-consensus regimes. Broad regimes that include high-consensus predictions require extremely high leader-override precision, while uncertainty-focused regimes are more favorable. Excluding high-consensus predictions yields a more than twofold reduction in required false-override suppression (from $\\approx 2.4$ to $\\approx 1.0$), a difference that is statistically significant ($p < 10^{-6}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6b: top-2 flip subset discovery (strict mode)\n",
    "matrix, rectangles, threshold_rectangles, tie_stats = analysis.top2_flip_analysis(\n",
    "    df_branches, strict=True, min_support=10\n",
    ")\n",
    "threshold_rectangles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6c: top-2 flip playground (absolute + relative ranges)\n",
    "analysis.top2_flip_playground(\n",
    "    df_branches, top1_votes_min=6, top1_votes_max=10, top2_votes_min=1, top2_votes_max=7\n",
    ")\n",
    "analysis.top2_flip_playground_relative(\n",
    "    df_branches, top1_votes_min=6, top1_votes_max=10, gap_min=0, gap_max=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "> Figure 2: Feasibility of selective top-2 leader override across vote-consensus regimes. The x-axis shows the maximum overall accuracy achievable by an ideal oracle that corrects all top-2=gold cases within a regime, and the y-axis indicates the required false-override suppression (top-1 correct vs. top-2 correct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6d: top-2 flip subset discovery (relative gap search)\n",
    "matrix_gap, rectangles_gap, threshold_rectangles_gap, tie_stats_gap = \\\n",
    "analysis.top2_flip_analysis_relative(\n",
    "    df_branches, strict=True, min_support=10\n",
    ")\n",
    "threshold_rectangles_gap\n",
    "\n",
    "# Figure 2: selective top-2 flip feasibility\n",
    "from rofa.analysis.plots import plot_top2_flip_feasibility\n",
    "\n",
    "baseline_acc = float(df_leader_accuracy[\"value\"].iloc[0])\n",
    "fig, ax, plot_df = plot_top2_flip_feasibility(\n",
    "    rectangles_gap,\n",
    "    baseline_acc,\n",
    "    total_n=len(df_branches),\n",
    "    use_frontier_df=threshold_rectangles_gap,\n",
    "    save_path=\"figure2_top2_flip_feasibility.png\",\n",
    ")\n",
    "plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6e: sensitivity analysis around relative gap selections\n",
    "gap_neighbors = analysis.make_gap_neighbor_rows(\n",
    "    df_branches, threshold_rectangles_gap, include_gap_min_neighbors=False\n",
    ")\n",
    "gap_neighbors[[\n",
    "    \"source_row\",\"variant\",\n",
    "    \"top1_votes_min\",\"top1_votes_max\",\"gap_min\",\"gap_max\",\n",
    "    \"total_examples_count\",\n",
    "    \"total_top1_correct_count\",\"total_top2_correct_count\",\n",
    "    \"harm_to_benefit_ratio\",\n",
    "    \"always_flip_delta_accuracy\",\"delta_always_flip_delta_accuracy\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Supporting Tables and Diagnostics\n",
    "\n",
    "The following tables are supporting artifacts referenced in paper exports and exploratory diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "# R3: distribution of max_frac\n",
    "df_max_frac = analysis.max_frac_distribution(df_branches).reset_index()\n",
    "df_max_frac.columns = [\"max_frac_bin\", \"count\"]\n",
    "df_max_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "# R7: R/W/Other breakdown by max_frac bins\n",
    "df_rw_other = analysis.rw_other_breakdown(df_branches)\n",
    "df_rw_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "# R8: error modes (unanimous wrong)\n",
    "df_unanimous_wrong = analysis.unanimous_wrong(df_branches)\n",
    "df_unanimous_wrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "# R10: subject-wise breakdown (optional)\n",
    "df_subject_breakdown = notebook_helpers.subject_breakdown(df_greedy, df_branches)\n",
    "df_subject_breakdown.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": [
    "# R11: export paper tables\n",
    "report_dir = notebook_helpers.export_paper_reports(\n",
    "    metadata,\n",
    "    df_greedy_accuracy,\n",
    "    df_leader_accuracy,\n",
    "    unanimous_stats,\n",
    "    near_unanimous_stats,\n",
    "    df_top2,\n",
    "    df_max_frac,\n",
    "    df_rw_other,\n",
    "    df_subject_breakdown,\n",
    ")\n",
    "print(\"Saved reports to\", report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "16"
   },
   "source": [
    "## Add your own analysis below\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
