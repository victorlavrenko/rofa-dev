{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Reproducing \"From Answers to Hypotheses\"\n",
    "\n",
    "This notebook reproduces all figures and statistical analyses reported in:\n",
    "\n",
    "> Victor Lavrenko. *From Answers to Hypotheses: Internal Consensus and Its Limits in Large Language Models*. 2026.\n",
    "\n",
    "The results correspond to the tagged release:\n",
    "`paper/from-answers-to-hypotheses-v1`\n",
    "\n",
    "Repository:\n",
    "https://github.com/victorlavrenko/rofa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the ROFA package and load the precomputed runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "# install ROFA package if not already installed\n",
    "import importlib.metadata\n",
    "\n",
    "try: \n",
    "  importlib.metadata.distribution(\"rofa\")\n",
    "except importlib.metadata.PackageNotFoundError: \n",
    "  from pathlib import Path\n",
    "  if (Path.cwd().parent.parent / \"pyproject.toml\").is_file():\n",
    "      %pip install -e \"../..\"\n",
    "  else:\n",
    "      if not Path(\"rofa\").is_dir():\n",
    "          !git clone https://github.com/victorlavrenko/rofa\n",
    "      %pip install -e \"rofa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"rofa.papers\") is None:\n",
    "    print(\n",
    "        \"\\n⚠️  Runtime restart required\\n\\n\"\n",
    "        \"ROFA has just been installed, but the Python runtime has not been restarted yet.\\n\\n\"\n",
    "        \"Please restart the runtime via:\\n\"\n",
    "        \"  Runtime (or ▼ after Run all) → Restart runtime (or Restart runtime and run all)\\n\"\n",
    "        \"This is expected behaviour in Google Colab.\"\n",
    "    )\n",
    "    raise SystemExit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rofa.papers.from_answers_to_hypotheses import analysis, notebook_helpers\n",
    "\n",
    "# Get run artifacts\n",
    "run_dir_greedy, greedy_asset_url = (\n",
    "    r\"\",\n",
    "    \"https://github.com/victorlavrenko/rofa/releases/download/paper%2Ffrom-answers-to-hypotheses-v1/rofa-from-answers-to-hypotheses-runs-v1-greedy.zip\",\n",
    ")\n",
    "run_dir_k_sample, k_sample_asset_url = (\n",
    "    r\"\",\n",
    "    \"https://github.com/victorlavrenko/rofa/releases/download/paper%2Ffrom-answers-to-hypotheses-v1/rofa-from-answers-to-hypotheses-runs-v1-branches10.zip\",\n",
    ")\n",
    "run_inputs = notebook_helpers.resolve_run_inputs(\n",
    "    run_dir_greedy, greedy_asset_url, run_dir_k_sample, k_sample_asset_url\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "a135c5f2-3bae-4864-d717-3ba79fba7c28"
   },
   "outputs": [],
   "source": [
    "# Load + validate\n",
    "df_greedy, df_branches, metadata = analysis.load_paper_runs(run_inputs)\n",
    "notebook_helpers.validate_required_columns(df_greedy, df_branches)\n",
    "notebook_helpers.print_run_summary(df_greedy, df_branches, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## H1: Aggregation Improves Accuracy\n",
    "\n",
    "> Greedy accuracy: 65.75%\n",
    "> Majority accuracy: 66.75%\n",
    "> A two-sided binomial test with null hypothesis $H_0: \\pi = 0.6575$ yields a p-value of approximately 0.63, indicating no statistically significant difference between greedy and majority-vote accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "# R1: greedy accuracy\n",
    "df_greedy_accuracy = pd.DataFrame(\n",
    "    {\"metric\": [\"greedy_accuracy\"], \"value\": [analysis.accuracy_greedy(df_greedy)]}\n",
    ")\n",
    "df_greedy_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "# R2: leader accuracy\n",
    "df_leader_accuracy = pd.DataFrame(\n",
    "    {\"metric\": [\"leader_accuracy\"], \"value\": [analysis.accuracy_leader(df_branches)]}\n",
    ")\n",
    "df_leader_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "# R9: majority vote does not help (greedy vs leader)\n",
    "df_majority_vote = notebook_helpers.majority_vote_table(df_greedy, df_branches)\n",
    "df_majority_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## H2: Correct Answers Appear Among Alternatives\n",
    "\n",
    "> Observed Top-2 coverage is 80.5%, compared to a greedy accuracy of 65.75%, corresponding to an absolute improvement of 14.75 percentage points. Using a binomial model with null hypothesis $H_0: \\pi = 0.6575$, this difference is highly statistically significant ($p$-value $\\ll 10^{-6}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "# R6: top-2 coverage\n",
    "df_top2 = analysis.compute_table_top2(df_branches)\n",
    "df_top2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## H3: Internal Consensus Implies Correctness\n",
    "\n",
    "> Unanimous cases: 151\n",
    "> Unanimous accuracy: 86.8%\n",
    "> A one-sided binomial test of $H_0: \\pi \\ge 0.95$ yields a p-value below 0.01, so even strong internal consensus does not guarantee near-perfect reliability.\n",
    "> Near-unanimous cases ($\\text{max\\_frac} \\ge 0.9$) still exhibit error rates above 15%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "# R4: unanimous stats\n",
    "unanimous_stats = analysis.unanimous_stats(df_branches)\n",
    "df_unanimous = pd.DataFrame([unanimous_stats])\n",
    "df_unanimous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "9"
   },
   "outputs": [],
   "source": [
    "# R5: near-unanimous stats\n",
    "near_unanimous_stats = analysis.near_unanimous_stats(df_branches, threshold=0.9)\n",
    "df_near_unanimous = pd.DataFrame([near_unanimous_stats])\n",
    "df_near_unanimous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R5b: operational failure-mode breakdown (top-2 + unanimity)\n",
    "from collections import Counter\n",
    "\n",
    "def _pick_col(df, candidates):\n",
    "    for name in candidates:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    raise KeyError(f\"Missing required columns. Tried: {candidates}\")\n",
    "\n",
    "gold_col = _pick_col(df_branches, [\"gold\", \"answer\", \"label\"])\n",
    "leader_col = _pick_col(df_branches, [\"leader\", \"majority\", \"leader_answer\"])\n",
    "branch_col = _pick_col(df_branches, [\"branch_preds\", \"branches\", \"preds\"])\n",
    "max_frac_col = next((c for c in [\"max_frac\", \"max_frac_exact\"] if c in df_branches.columns), None)\n",
    "leader_correct_col = next((c for c in [\"leader_correct\", \"correct\"] if c in df_branches.columns), None)\n",
    "\n",
    "leader_correct = (\n",
    "    df_branches[leader_correct_col].fillna(False).astype(bool)\n",
    "    if leader_correct_col\n",
    "    else (df_branches[leader_col] == df_branches[gold_col]).fillna(False)\n",
    ")\n",
    "leader_wrong = ~leader_correct\n",
    "\n",
    "def _top_k(preds, k=2):\n",
    "    # Tie-break: rank by count, then by earliest first appearance in branch_preds.\n",
    "    preds_clean = [p for p in preds if p is not None]\n",
    "    if not preds_clean:\n",
    "        return []\n",
    "    first_idx = {}\n",
    "    for i, p in enumerate(preds):\n",
    "        if p is None:\n",
    "            continue\n",
    "        if p not in first_idx:\n",
    "            first_idx[p] = i\n",
    "    counts = Counter(preds_clean)\n",
    "    ranked = sorted(counts.items(), key=lambda kv: (-kv[1], first_idx[kv[0]]))\n",
    "    return [item[0] for item in ranked[:k]]\n",
    "\n",
    "gold_series = df_branches[gold_col]\n",
    "branch_series = df_branches[branch_col]\n",
    "gold_in_top2 = [\n",
    "    gold in _top_k(preds, k=2)\n",
    "    for preds, gold in zip(branch_series, gold_series, strict=False)\n",
    "]\n",
    "\n",
    "if max_frac_col:\n",
    "    unanimous_mask = df_branches[max_frac_col].fillna(0.0) == 1.0\n",
    "else:\n",
    "    unanimous_mask = [\n",
    "        len({p for p in preds if p is not None}) == 1\n",
    "        for preds in branch_series\n",
    "    ]\n",
    "\n",
    "n_total = len(df_branches)\n",
    "n_errors = int(leader_wrong.sum())\n",
    "selection_mask = leader_wrong & pd.Series(gold_in_top2)\n",
    "unsurfaced_mask = leader_wrong & ~pd.Series(gold_in_top2)\n",
    "\n",
    "sel_n = int(selection_mask.sum())\n",
    "uns_n = int(unsurfaced_mask.sum())\n",
    "\n",
    "def _pct(num, denom):\n",
    "    return 0.0 if denom == 0 else 100.0 * num / denom\n",
    "\n",
    "sel_total_pct = _pct(sel_n, n_total)\n",
    "sel_error_pct = _pct(sel_n, n_errors)\n",
    "uns_total_pct = _pct(uns_n, n_total)\n",
    "uns_error_pct = _pct(uns_n, n_errors)\n",
    "\n",
    "unanimous_mask = pd.Series(unanimous_mask)\n",
    "unanim_n = int(unanimous_mask.sum())\n",
    "unanim_wrong_n = int((unanimous_mask & leader_wrong).sum())\n",
    "unanim_wrong_pct = _pct(unanim_wrong_n, unanim_n)\n",
    "unanim_acc_pct = 100.0 - unanim_wrong_pct if unanim_n else 0.0\n",
    "unanim_share_errors_pct = _pct(unanim_wrong_n, n_errors)\n",
    "\n",
    "print(\"Failure mode breakdown (N=10 sampling)\")\n",
    "print(f\"N_total: {n_total}\")\n",
    "print(f\"N_errors: {n_errors}\")\n",
    "print(\n",
    "    f\"selection_errors: {sel_n} (\"\n",
    "    f\"{sel_total_pct:.1f}% total, {sel_error_pct:.1f}% of errors)\"\n",
    ")\n",
    "print(\n",
    "    f\"unsurfaced_errors: {uns_n} (\"\n",
    "    f\"{uns_total_pct:.1f}% total, {uns_error_pct:.1f}% of errors)\"\n",
    ")\n",
    "print(\n",
    "    f\"unanimous: {unanim_n} (acc {unanim_acc_pct:.1f}%, \"\n",
    "    f\"wrong {unanim_wrong_n}, wrong rate {unanim_wrong_pct:.1f}%, \"\n",
    "    f\"share of errors {unanim_share_errors_pct:.1f}%)\"\n",
    ")\n",
    "\n",
    "if max_frac_col:\n",
    "    near_mask = df_branches[max_frac_col].fillna(0.0) >= 0.9\n",
    "    near_n = int(near_mask.sum())\n",
    "    near_wrong_n = int((near_mask & leader_wrong).sum())\n",
    "    near_wrong_pct = _pct(near_wrong_n, near_n)\n",
    "    print(\n",
    "        f\"near_unanimous>=0.9: {near_n} (wrong {near_wrong_n}, \"\n",
    "        f\"wrong rate {near_wrong_pct:.1f}%)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "> Figure 1: Accuracy as a function of internal consensus (max_frac). Higher branch agreement correlates with higher accuracy, but even near-unanimous predictions exhibit a non-zero error rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "7"
   },
   "outputs": [],
   "source": [
    "# Figure 1: accuracy vs internal consensus (max_frac_exact)\n",
    "df_max_frac_exact = notebook_helpers.plot_accuracy_vs_consensus(\n",
    "    df_branches, \"figure1_max_frac_exact.png\"\n",
    ")\n",
    "df_max_frac_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## H4: Selective Leader Override Is Feasible\n",
    "\n",
    "> We analyzed selective top-2 leader override across vote-consensus regimes. Broad regimes that include high-consensus predictions require extremely high leader-override precision, while uncertainty-focused regimes are more favorable. Excluding high-consensus predictions yields a more than twofold reduction in required false-override suppression (from $\\approx 2.4$ to $\\approx 1.0$), a difference that is statistically significant ($p < 10^{-6}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6b: top-2 flip subset discovery (strict mode)\n",
    "matrix, rectangles, threshold_rectangles, tie_stats = analysis.top2_flip_analysis(\n",
    "    df_branches, strict=True, min_support=10\n",
    ")\n",
    "threshold_rectangles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6c: top-2 flip playground (absolute + relative ranges)\n",
    "analysis.top2_flip_playground(\n",
    "    df_branches, top1_votes_min=6, top1_votes_max=10, top2_votes_min=1, top2_votes_max=7\n",
    ")\n",
    "analysis.top2_flip_playground_relative(\n",
    "    df_branches, top1_votes_min=6, top1_votes_max=10, gap_min=0, gap_max=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "> Figure 2: Feasibility of selective top-2 leader override across vote-consensus regimes. The x-axis shows the maximum overall accuracy achievable by an ideal oracle that corrects all top-2=gold cases within a regime, and the y-axis indicates the required false-override suppression (top-1 correct vs. top-2 correct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6d: top-2 flip subset discovery (relative gap search)\n",
    "matrix_gap, rectangles_gap, threshold_rectangles_gap, tie_stats_gap = \\\n",
    "analysis.top2_flip_analysis_relative(\n",
    "    df_branches, strict=True, min_support=10\n",
    ")\n",
    "threshold_rectangles_gap\n",
    "\n",
    "# Figure 2: selective top-2 flip feasibility\n",
    "from rofa.analysis.plots import plot_top2_flip_feasibility\n",
    "\n",
    "baseline_acc = float(df_leader_accuracy[\"value\"].iloc[0])\n",
    "fig, ax, plot_df = plot_top2_flip_feasibility(\n",
    "    rectangles_gap,\n",
    "    baseline_acc,\n",
    "    total_n=len(df_branches),\n",
    "    use_frontier_df=threshold_rectangles_gap,\n",
    "    save_path=\"figure2_top2_flip_feasibility.png\",\n",
    ")\n",
    "plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6e: sensitivity analysis around relative gap selections\n",
    "gap_neighbors = analysis.make_gap_neighbor_rows(\n",
    "    df_branches, threshold_rectangles_gap, include_gap_min_neighbors=False\n",
    ")\n",
    "gap_neighbors[[\n",
    "    \"source_row\",\"variant\",\n",
    "    \"top1_votes_min\",\"top1_votes_max\",\"gap_min\",\"gap_max\",\n",
    "    \"total_examples_count\",\n",
    "    \"total_top1_correct_count\",\"total_top2_correct_count\",\n",
    "    \"harm_to_benefit_ratio\",\n",
    "    \"always_flip_delta_accuracy\",\"delta_always_flip_delta_accuracy\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Supporting Tables and Diagnostics\n",
    "\n",
    "The following tables are supporting artifacts referenced in paper exports and exploratory diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "# R3: distribution of max_frac\n",
    "df_max_frac = analysis.max_frac_distribution(df_branches).reset_index()\n",
    "df_max_frac.columns = [\"max_frac_bin\", \"count\"]\n",
    "df_max_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "# R7: R/W/Other breakdown by max_frac bins\n",
    "df_rw_other = analysis.rw_other_breakdown(df_branches)\n",
    "df_rw_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "# R8: error modes (unanimous wrong)\n",
    "df_unanimous_wrong = analysis.unanimous_wrong(df_branches)\n",
    "df_unanimous_wrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "# R10: subject-wise breakdown (optional)\n",
    "df_subject_breakdown = notebook_helpers.subject_breakdown(df_greedy, df_branches)\n",
    "df_subject_breakdown.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": [
    "# R11: export paper tables\n",
    "report_dir = notebook_helpers.export_paper_reports(\n",
    "    metadata,\n",
    "    df_greedy_accuracy,\n",
    "    df_leader_accuracy,\n",
    "    unanimous_stats,\n",
    "    near_unanimous_stats,\n",
    "    df_top2,\n",
    "    df_max_frac,\n",
    "    df_rw_other,\n",
    "    df_subject_breakdown,\n",
    ")\n",
    "print(\"Saved reports to\", report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "16"
   },
   "source": [
    "## Add your own analysis below\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
